# AI_Weather_Benchmarking
Problem Overview
Numerical weather prediction is peculiar in a way that surprises many folks outside of meteorology - rather than just produce the specific, precise forecast for something that a user might care about, these models instead simulate the entire atmosphere. Forecasts actually have to be backed out of the model output; raw model output lightly processed for use as a forecast is often called “guidance,” and comes with no guarantees about accuracy or bias.

ML weather models such as Google DeepMind’s GraphCast or NVIDIA’s FourCastNet emulate these numerical weather prediction systems, producing a subset of output that, essentially, simulates the whole atmosphere. So using them to create weather forecasts suffers the same problem as the numerical models - and that problem further extends to evaluating these models’ accuracy and performance.

Despite this problem, the ML-weather research community has made great strides in evaluating their models through projects like WeatherBench. But WeaterBench and similar evaluation approaches have a fatal flaw when it comes to understanding how well ML weather models perform for forecasting applications: they’re not evaluated the way an end user might assess the forecast, which is focused on the specific performance of the model relative to a restricted set of observations for a particular forecast scenario. This is especially the case for extreme weather events, which may feature unique biases or failure modes that general, large-scale evaluation of the mean forecast fails to capture.
